{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkkr1HUBPOcL"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.883 · Aprendizaje por refuerzo</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster universitario en Ciencia de datos (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo: Método _TD learning_ en el entorno WindyGridWorld\n",
    "\n",
    "En este ejemplo implementaremos el método _TD learning_ de aprendizaje por refuerzo para buscar una solución óptima en el problema de WindyGridWorld."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyFUy1qlDGku"
   },
   "source": [
    "## 1. El entorno __WindyGridWorld__\n",
    "\n",
    "El entorno __WindyGridWorld__ consiste en un agente que se mueve en una cuadrícula 7x10 (alto x ancho). En cada paso, el agente tiene 4 opciones de acción o movimiento: ARRIBA, ABAJO, DERECHA, IZQUIERDA. El agente siempre sale de la misma casilla [3, 0] y el juego termina cuando el agente llega a la casilla de llegada [3, 7]. \n",
    "\n",
    "El entorno se corresponde con el ejemplo 'Cuadrícula con viento' explicado en la sección 3.1.2. el módulo \"Métodos de Diferencia Temporal\". El problema radica en que hay un viento que empuja al agente hacia arriba en la parte central de la cuadrícula. Esto provoca que, aunque se ejecute una acción estándar, en la región central los estados resultantes se desplazan hacia arriba por un viento cuya fuerza varía entre columnas.\n",
    "\n",
    "<img src=\"../figs/GridWorld.png\">\n",
    "\n",
    "El código para implementar este entorno, que se encuentra disponible en el fichero adjunto `windy_gridworld_env.py`, ha sido adaptado del siguiente enlace:\n",
    "\n",
    "https://pypi.org/project/gym-gridworlds/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Métodos de Diferencia Temporal\n",
    "\n",
    "El objetivo de este ejercicio es realizar una estimación de la política óptima mediante los métodos de Diferencia Temporal en el entorno WindyGridWorld."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, implementaremos el algoritmo de *Q-learning* explicado en el modulo \"Aprendizaje por Diferencia Temporal\" utilizando los siguientes parámetros:\n",
    "    \n",
    "- número de episodios = 200\n",
    "- *learning rate* = 0.5\n",
    "- *discount factor* = 1\n",
    "- *epsilon* = 0.05    \n",
    "\n",
    "Además, queremos que la solución propuesta nos permita: \n",
    "\n",
    "1. Mostrar por pantalla los valores Q estimados para cada estado. \n",
    "2. Mostrar por pantalla los valores de la función de valor $v_\\pi(s)$ estimada para cada estado. \n",
    "3. Ejecutar un episodio siguiendo la política óptima encontrada, donde se pueda reconocer la trayectoria seguida por el agente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(4) \n",
      "Observation space is Tuple(Discrete(7), Discrete(10)) \n",
      "Reward range is (-inf, inf) \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mcol\n",
    "from matplotlib.legend_handler import HandlerLineCollection, HandlerTuple\n",
    "from matplotlib.lines import Line2D\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "import numpy as np\n",
    "import windy_gridworld_env as wge\n",
    "\n",
    "env = wge.WindyGridworldEnv()\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))\n",
    "print(\"Reward range is {} \".format(env.reward_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, state, nA, epsilon):\n",
    "    '''\n",
    "    Create a policy in which epsilon dictates how likely it will \n",
    "    take a random action.\n",
    "\n",
    "    :param Q: links state -> action value (dictionary)\n",
    "    :param state: state character is in (int)\n",
    "    :param nA: number of actions (int)\n",
    "    :param epsilon: chance it will take a random move (float)\n",
    "    :return: probability of each action to be taken (list)\n",
    "    '''\n",
    "    probs = np.ones(nA) * epsilon / nA\n",
    "    best_action = np.argmax(Q[state])\n",
    "    probs[best_action] += 1.0 - epsilon\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(episodes, learning_rate, discount, epsilon):\n",
    "    '''\n",
    "    Learn to solve the environment using Q-learning\n",
    "\n",
    "    :param episodes: Number of episodes to run (int)\n",
    "    :param learning_rate: How fast it will converge to a point (float [0, 1])\n",
    "    :param discount: How much future events lose their value (float [0, 1])\n",
    "    :param epsilon: chance a random move is selected (float [0, 1])\n",
    "    :return: x,y points to graph\n",
    "    '''\n",
    "\n",
    "    # Links state to action values\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    # Points to plot\n",
    "    # number of episodes\n",
    "    x = np.arange(episodes)\n",
    "    # Number of steps\n",
    "    y = np.zeros(episodes)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "                \n",
    "        for step in range(10000):\n",
    "\n",
    "            # Select and take action\n",
    "            probs = epsilon_greedy_policy(Q, state, env.action_space.n, epsilon)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "           \n",
    "            # TD Update\n",
    "            td_target = reward + discount * np.amax(Q[next_state])\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += learning_rate * td_error\n",
    "                        \n",
    "            if done:\n",
    "                y[episode] = step\n",
    "                break\n",
    "\n",
    "            state = next_state   \n",
    "                       \n",
    "    return x, y, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, q = Q_learning(episodes=200, learning_rate=0.5, discount=1, epsilon=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) [-13.5        -13.56817121 -13.99884244 -13.42727579] -13.427275786779001\n",
      "(0, 1) [-13.35708173 -13.42477065 -13.47805434 -13.3875045 ] -13.357081732283543\n",
      "(0, 2) [-13.1960478  -12.9086274  -13.24240882 -13.11640552] -12.908627395059405\n",
      "(0, 3) [-12.         -11.99598565 -12.         -12.8421897 ] -11.995985649026961\n",
      "(0, 4) [-11.63301402 -10.99999999 -11.69134837 -11.60436134] -10.999999990312016\n",
      "(0, 5) [-10.5        -10.         -10.5        -10.08108124] -10.0\n",
      "(0, 6) [ -9.49998088  -9.          -9.875      -10.71021865] -9.0\n",
      "(0, 7) [-8.87469142 -8.         -8.         -9.21932174] -8.0\n",
      "(0, 8) [-7.98046875 -7.         -7.36194229 -8.52707291] -7.0\n",
      "(0, 9) [-6.3198061  -6.9375     -6.         -7.96594195] -6.0\n",
      "(1, 0) [-13.91727947 -14.06284124 -14.32849942 -13.94065094] -13.917279473729955\n",
      "(1, 1) [-13.62623572 -13.25506492 -13.85403354 -13.76927917] -13.255064916352932\n",
      "(1, 2) [-13.20183347 -12.88266899 -12.93578341 -12.9130946 ] -12.882668989094089\n",
      "(1, 3) [-12.15379363 -11.99835672 -12.39500955 -12.27291368] -11.998356724526023\n",
      "(1, 4) [-11.2782335  -10.9999781  -11.44031453 -11.8683985 ] -10.999978097367912\n",
      "(1, 5) [-10.95326662 -10.         -10.875      -11.36149548] -10.0\n",
      "(1, 6) [-9.44504096 -8.99997119 -9.05103475 -9.73695498] -8.999971190271443\n",
      "(1, 7) [-8.45145517 -7.97379047 -8.34259382 -8.90672631] -7.973790467284407\n",
      "(1, 8) [-6.91478286 -6.2338796  -6.         -7.3183688 ] -6.0\n",
      "(1, 9) [-6.51136485 -5.97949219 -5.         -6.41570979] -5.0\n",
      "(2, 0) [-14.57176263 -14.35608566 -14.85613559 -14.5       ] -14.356085658886396\n",
      "(2, 1) [-13.79786745 -13.85605344 -14.31226951 -13.94828887] -13.797867451878371\n",
      "(2, 2) [-13.226924   -12.99367064 -13.31166521 -13.65845154] -12.993670641234456\n",
      "(2, 3) [-12.40641672 -11.99938105 -12.42216093 -12.76690817] -11.999381052320519\n",
      "(2, 4) [-11.91252953 -11.         -11.9290737  -12.67153524] -11.0\n",
      "(2, 5) [-10.32778737  -9.99932694 -10.         -10.95170186] -9.999326944042496\n",
      "(2, 6) [-9.59156753 -8.99729714 -9.45418761 -9.13675007] -8.997297135382432\n",
      "(2, 7) [-4.99246216 -4.37356742 -4.59863952 -6.75      ] -4.373567420712789\n",
      "(2, 8) [-6.60026415 -5.98481402 -6.         -7.35482887] -5.984814017377552\n",
      "(2, 9) [-5.86430169 -4.5        -4.         -6.59321125] -4.0\n",
      "(3, 0) [-15.04942037 -15.         -15.08291053 -15.22202409] -15.0\n",
      "(3, 1) [-14.54191106 -14.         -14.26785514 -15.57441288] -14.0\n",
      "(3, 2) [-13.70501376 -13.         -13.63948352 -13.51102485] -13.0\n",
      "(3, 3) [-12.52097122 -12.         -12.4375     -13.68552539] -12.0\n",
      "(3, 4) [-11.06432611 -10.99358713 -11.19660823 -11.83063097] -10.993587127622849\n",
      "(3, 5) [-10.31424683  -9.97405419 -10.37340124 -10.74798705] -9.974054191296231\n",
      "(3, 6) [-9.27612034 -8.86015594 -9.31390967 -9.63297452] -8.860155935505194\n",
      "(3, 7) [0. 0. 0. 0.] 0.0\n",
      "(3, 8) [-4.125      -3.31201422 -3.5        -3.77585793] -3.3120142221450806\n",
      "(3, 9) [-4.71500993 -3.75       -3.         -3.18115234] -3.0\n",
      "(4, 0) [-14.76552849 -14.22375864 -14.30168017 -14.5       ] -14.223758638078586\n",
      "(4, 1) [-13.63751208 -13.62145585 -13.64914147 -13.8749993 ] -13.621455846373713\n",
      "(4, 2) [-13.38633651 -12.85639314 -13.02535268 -13.49839969] -12.856393144298996\n",
      "(4, 3) [-12.26624225 -11.96535821 -12.29095191 -11.97508021] -11.965358206385922\n",
      "(4, 4) [-11.01608602 -10.88251237 -10.81434225 -10.90524395] -10.814342245459557\n",
      "(4, 5) [-10.13914771  -9.67076991 -10.         -10.05826436] -9.670769914301834\n",
      "(4, 6) [0. 0. 0. 0.] 0.0\n",
      "(4, 7) [-2.6875     -2.15229061 -0.5         0.        ] 0.0\n",
      "(4, 8) [-6.69416797 -3.29296875 -1.74999809 -1.        ] -1.0\n",
      "(4, 9) [-3.94352722 -2.875      -2.37426758 -2.        ] -2.0\n",
      "(5, 0) [-14.04963792 -13.61470091 -13.72202294 -13.90182955] -13.614700909540652\n",
      "(5, 1) [-13.07066049 -13.07108666 -12.9940333  -13.22319037] -12.9940332969332\n",
      "(5, 2) [-12.4620282  -12.4534565  -12.33946809 -12.90559135] -12.339468091290593\n",
      "(5, 3) [-11.89085377 -11.71358538 -11.89026859 -12.41544511] -11.713585379630633\n",
      "(5, 4) [-10.5876532  -10.45112408 -10.76401937 -11.48073189] -10.451124082432397\n",
      "(5, 5) [0. 0. 0. 0.] 0.0\n",
      "(5, 6) [0. 0. 0. 0.] 0.0\n",
      "(5, 7) [-2.03879547 -2.          0.          0.        ] 0.0\n",
      "(5, 8) [-2.       -1.203125 -1.       -0.875   ] -0.875\n",
      "(5, 9) [-1.96484375 -2.         -1.703125   -1.546875  ] -1.546875\n",
      "(6, 0) [-13.12965653 -13.3082839  -13.25       -13.44401713] -13.129656527614696\n",
      "(6, 1) [-12.66146842 -12.77163424 -13.02131682 -12.89351058] -12.661468419427386\n",
      "(6, 2) [-12.21020163 -12.06370357 -12.4219827  -12.60969866] -12.063703567027922\n",
      "(6, 3) [-11.19509894 -11.2983147  -11.375      -11.2818538 ] -11.195098938221106\n",
      "(6, 4) [0. 0. 0. 0.] 0.0\n",
      "(6, 5) [0. 0. 0. 0.] 0.0\n",
      "(6, 6) [0. 0. 0. 0.] 0.0\n",
      "(6, 7) [0. 0. 0. 0.] 0.0\n",
      "(6, 8) [-0.75    -1.09375 -1.      -0.75   ] -0.75\n",
      "(6, 9) [-1.125   -1.65625 -1.      -1.1875 ] -1.0\n"
     ]
    }
   ],
   "source": [
    "#mostrar los Q de cada estado-acción y la función valor\n",
    "\n",
    "f = open('output.txt', 'w')\n",
    "for i in range(7):\n",
    "    for j in range(10):\n",
    "        V = max(q[i,j])    \n",
    "        print((i,j), q[i,j], V)\n",
    "        print((i,j), q[i,j], V, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función que muestra los valores de la función de estado V(s) en la cuadricula\n",
    "\n",
    "def print_values(Q, height, width):\n",
    "    for i in range(height):\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        for j in range(width):\n",
    "            arr = np.array(Q[i,j])\n",
    "            v = np.amax(arr)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "-13.43|-13.36|-12.91|-12.00|-11.00|-10.00|-9.00|-8.00|-7.00|-6.00|\n",
      "------------------------------------------------------------------------------------------\n",
      "-13.92|-13.26|-12.88|-12.00|-11.00|-10.00|-9.00|-7.97|-6.00|-5.00|\n",
      "------------------------------------------------------------------------------------------\n",
      "-14.36|-13.80|-12.99|-12.00|-11.00|-10.00|-9.00|-4.37|-5.98|-4.00|\n",
      "------------------------------------------------------------------------------------------\n",
      "-15.00|-14.00|-13.00|-12.00|-10.99|-9.97|-8.86| 0.00|-3.31|-3.00|\n",
      "------------------------------------------------------------------------------------------\n",
      "-14.22|-13.62|-12.86|-11.97|-10.81|-9.67| 0.00| 0.00|-1.00|-2.00|\n",
      "------------------------------------------------------------------------------------------\n",
      "-13.61|-12.99|-12.34|-11.71|-10.45| 0.00| 0.00| 0.00|-0.88|-1.55|\n",
      "------------------------------------------------------------------------------------------\n",
      "-13.13|-12.66|-12.06|-11.20| 0.00| 0.00| 0.00| 0.00|-0.75|-1.00|\n"
     ]
    }
   ],
   "source": [
    "print_values(q, 7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ejecución de un episodio siguiendo la politica optima\n",
    "\n",
    "def execute_episode_TD(q, env):\n",
    "    obs = env.reset()\n",
    "    t, total_reward, done = 0, 0, False\n",
    "\n",
    "    print(\"Obs inicial: {} \".format(obs))\n",
    "\n",
    "    switch_action = {\n",
    "            0: \"U\",\n",
    "            1: \"R\",\n",
    "            2: \"D\",\n",
    "            3: \"L\",\n",
    "        }\n",
    "\n",
    "    for t in range(1000): # limitamos el número de time-steps de cada episodio a 1000\n",
    "        \n",
    "        # Elegir una acción siguiendo la política óptima\n",
    "        arr = np.array(q[obs])\n",
    "        action = arr.argmax()\n",
    "       \n",
    "        # Ejecutar la acción y esperar la respuesta del entorno\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        obs = new_obs\n",
    "        print(\"Action: {} -> Obs: {} and reward: {}\".format(switch_action[action], obs, reward))\n",
    "\n",
    "        if t==999:\n",
    "            print(\"Number of time-septs exceeds 1000. STOP episode.\") \n",
    "        total_reward += reward\n",
    "        t += 1\n",
    "        if done:\n",
    "            break\n",
    "   \n",
    "    print(\"Episode finished after {} timesteps and reward was {} \".format(t, total_reward))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs inicial: (3, 0) \n",
      "Action: R -> Obs: (3, 1) and reward: -1\n",
      "Action: R -> Obs: (3, 2) and reward: -1\n",
      "Action: R -> Obs: (3, 3) and reward: -1\n",
      "Action: R -> Obs: (2, 4) and reward: -1\n",
      "Action: R -> Obs: (1, 5) and reward: -1\n",
      "Action: R -> Obs: (0, 6) and reward: -1\n",
      "Action: R -> Obs: (0, 7) and reward: -1\n",
      "Action: R -> Obs: (0, 8) and reward: -1\n",
      "Action: R -> Obs: (0, 9) and reward: -1\n",
      "Action: D -> Obs: (1, 9) and reward: -1\n",
      "Action: D -> Obs: (2, 9) and reward: -1\n",
      "Action: D -> Obs: (3, 9) and reward: -1\n",
      "Action: D -> Obs: (4, 9) and reward: -1\n",
      "Action: L -> Obs: (4, 8) and reward: -1\n",
      "Action: L -> Obs: (3, 7) and reward: -1\n",
      "Episode finished after 15 timesteps and reward was -15 \n"
     ]
    }
   ],
   "source": [
    "execute_episode_TD(q, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M2.883_PEC1_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
